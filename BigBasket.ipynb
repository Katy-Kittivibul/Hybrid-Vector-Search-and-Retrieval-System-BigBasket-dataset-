{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p7Z_46NmFZHc",
        "qxud1BAQHr8h"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPb8pclmYIpkoOBQ84F8uOP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Katy-Kittivibul/Hybrid-Vector-Search-and-Retrieval-System-BigBasket-dataset-/blob/main/BigBasket.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BigBasket**\n",
        "\n",
        "This project will learn how to implement natural language processing (NLP) in prediction or classification tasks. The project contains 3 main topics.\n",
        "\n",
        "1. Machine learning model\n",
        "2. Deep learning model\n",
        "3. Recommendation system\n",
        "\n",
        "\n",
        "\n",
        "URL: https://www.kaggle.com/datasets/surajjha101/bigbasket-entire-product-list-28k-datapoints/data"
      ],
      "metadata": {
        "id": "Da2cVentAyN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "XsKqPSJb76kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "id": "3bNPBB8fW-ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZQT_l2-7uLE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os, shutil, glob\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dataset"
      ],
      "metadata": {
        "id": "zSssczOC79-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"surajjha101/bigbasket-entire-product-list-28k-datapoints\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "h2aMwdDr7_8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_data = os.listdir(path)\n",
        "print(\"Files in the dataset folder:\", base_data)"
      ],
      "metadata": {
        "id": "yMQK4Vtf-zFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = base_data[0]\n",
        "data = os.path.join(path, csv_file)"
      ],
      "metadata": {
        "id": "TrB_DB4AALfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The path variable provided by kagglehub points to a folder (the dataset directory), not the file itself. You must use os.path.join() to combine this folder path with the actual CSV file name to create the full, readable file path"
      ],
      "metadata": {
        "id": "RV6WJn-aCtF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data cleaning"
      ],
      "metadata": {
        "id": "phhKn6LT8DPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "76nDkzzLAiPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "__Q0JIf8At0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "Np-wpWVRQiGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['product', 'brand'])\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "Vy_sW24r7AnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set columns for visualisation\n",
        "\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "categorial_cols = df.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "numerical_cols = numerical_cols.drop('index')\n",
        "categorial_cols = categorial_cols.drop('description')\n",
        "\n",
        "print(numerical_cols)\n",
        "print(categorial_cols)"
      ],
      "metadata": {
        "id": "nc0wCmFDT5se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "4jpjysYyIJL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical data"
      ],
      "metadata": {
        "id": "3RkJxsQhJY8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"category\"].value_counts()"
      ],
      "metadata": {
        "id": "nlJFfC6qJGUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_sum = df.groupby(\"category\")[[\"sub_category\", \"type\"]].value_counts().sort_values(ascending=False)\n",
        "print(categorical_sum)"
      ],
      "metadata": {
        "id": "Xcl5OfRBQjSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorial_df = pd.DataFrame(categorical_sum)\n",
        "display(categorial_df)"
      ],
      "metadata": {
        "id": "pLmmvkMSR6Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical data"
      ],
      "metadata": {
        "id": "lG-JjJRQJczF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.describe())"
      ],
      "metadata": {
        "id": "zk7h4W0s-q6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"rating\"].value_counts().sort_index(ascending=False)"
      ],
      "metadata": {
        "id": "I2VEmk1TSkvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data visualisation"
      ],
      "metadata": {
        "id": "p7Z_46NmFZHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising Numerical Columns"
      ],
      "metadata": {
        "id": "-DzYFKxUG1Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Histograms: to show the distribution of data"
      ],
      "metadata": {
        "id": "edrrr3AiHK60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_plots = len(numerical_cols)\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(15, 4))\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "  sns.histplot(df[col], ax=axes[i], kde=True)\n",
        "  axes[i].set_title(f'Distribution of {col}', fontsize=10)\n",
        "  axes[i].set_xlabel(col, fontsize=10)\n",
        "  axes[i].set_ylabel('Frequency', fontsize=10)\n",
        "  axes[i].tick_params(axis='both', labelsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iCB4rT3aVmtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Box plot: Identify the median, quartiles, and outliers."
      ],
      "metadata": {
        "id": "KifIajXxHTGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_plots = len(numerical_cols)\n",
        "fig, axes = plt.subplots(1, num_plots, figsize=(15, 4))\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "  sns.boxplot(df[col], ax=axes[i])\n",
        "  axes[i].set_title(f'Box plot of {col}', fontsize=10)\n",
        "  axes[i].set_xlabel(col, fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AmN7iK_wGhbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Pair plot: Show the relationship between numerical features"
      ],
      "metadata": {
        "id": "Xv7L7yw7ILcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[numerical_cols])"
      ],
      "metadata": {
        "id": "ghrivrurH-Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Heat map: Show the relationship between numerical features"
      ],
      "metadata": {
        "id": "FmrNI5YWISBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df[numerical_cols].corr())"
      ],
      "metadata": {
        "id": "9P7UQhw7IH6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualising Categorical Columns (Counts & Frequencies)"
      ],
      "metadata": {
        "id": "SNaEWdlLKTdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(y=df['category'], order=df['category'].value_counts().index, hue=df['category'], palette=\"mako\")"
      ],
      "metadata": {
        "id": "BxodmjIkKV1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_brands = df['brand'].value_counts().head(10)\n",
        "sns.barplot(x=top_brands.values, y=top_brands.index, hue=top_brands.index, palette='viridis')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Brand')\n",
        "plt.title('Top 10 Brands')"
      ],
      "metadata": {
        "id": "oNQQd4LdKjPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine learning"
      ],
      "metadata": {
        "id": "qxud1BAQHr8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis (PCA)"
      ],
      "metadata": {
        "id": "blVvgxKbPMnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df = df[['category', 'sale_price', 'market_price']].copy()\n",
        "pca_encoded = pd.get_dummies(pca_df, columns=['category'], drop_first=True) # turn categorical data into numerical data"
      ],
      "metadata": {
        "id": "bjSWocaoC5Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = pca_encoded\n",
        "scaler = StandardScaler()\n",
        "pca_scaled = scaler.fit_transform(pca) # standardisation\n",
        "pca_scaled_df = pd.DataFrame(pca_scaled, columns=pca.columns)"
      ],
      "metadata": {
        "id": "kL8H0tEAPYyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply PCA\n",
        "n_components = 2\n",
        "pca_model = PCA(n_components=n_components)\n",
        "pca_result = pca_model.fit_transform(pca_scaled_df)"
      ],
      "metadata": {
        "id": "RddaUQGsPuQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_cols = [f'PC{i+1}' for i in range(n_components)]\n",
        "pca_df = pd.DataFrame(pca_result, columns=pca_cols)\n",
        "print(\"PCA Resulting DataFrame (First 5 Rows):\")\n",
        "print(pca_df.head())"
      ],
      "metadata": {
        "id": "PJMSnlVkQL2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nExplained Variance Ratio for each Principal Component:\")\n",
        "for i, ratio in enumerate(pca_model.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f} ({ratio*100:.2f}% of total variance)\")"
      ],
      "metadata": {
        "id": "LDLjWBHMQXZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the original features (including one-hot encoded ones) to the components\n",
        "loadings = pd.DataFrame(pca_model.components_.T, columns=pca_cols, index=pca.columns)\n",
        "print(\"\\nComponent Loadings (Feature Contribution):\")\n",
        "print(loadings.abs().sort_values(by='PC1', ascending=False).head(5))"
      ],
      "metadata": {
        "id": "UqRXhaNLRDiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance_ratio = pca_model.explained_variance_ratio_\n",
        "components = range(1, len(explained_variance_ratio) + 1)\n",
        "\n",
        "# Create the Scree Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(components, explained_variance_ratio, color='skyblue')\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance Ratio by Principal Components')"
      ],
      "metadata": {
        "id": "u3hA3ilnSCvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add cumulative explained variance (optional but useful)\n",
        "cumulative_variance = explained_variance_ratio.cumsum()\n",
        "plt.plot(components, cumulative_variance, marker='o', linestyle='-', color='red', label='Cumulative Variance')\n",
        "plt.axhline(0.90, color='grey', linestyle=':', label='90% Target') # Highlight a 90% target line\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('pca_scree_plot.png')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "yemJGuaoTGDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning (BERT)"
      ],
      "metadata": {
        "id": "DTtHbTdc5vK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part will make the prediction model for \"category\" and \"sub-category\" sections when the user adds new product information. The BERT, a machine learning framework for natural language processing (NLP), will be implemented to classify language information to target categories."
      ],
      "metadata": {
        "id": "nsap5yKgS04z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set columns for making a model\n",
        "df_2 = df.copy()\n",
        "\n",
        "text_col = \"description\"\n",
        "cat_cols = [\"sub_category\", \"brand\", \"type\"]\n",
        "num_cols = [\"sale_price\", \"market_price\"]"
      ],
      "metadata": {
        "id": "5iLQ0O0Y5yws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode target\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "cat_encoder = LabelEncoder()\n",
        "subcat_encoder = LabelEncoder()\n",
        "\n",
        "df_2[\"cat_label\"] = cat_encoder.fit_transform(df_2[\"category\"])\n",
        "df_2[\"subcat_label\"] = subcat_encoder.fit_transform(df_2[\"sub_category\"])\n",
        "\n",
        "num_cat_labels = len(cat_encoder.classes_)\n",
        "num_subcat_labels = len(subcat_encoder.classes_)"
      ],
      "metadata": {
        "id": "-2PrAX1X8qsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test set\n",
        "train_dl_df, test_dl_df = train_test_split(df_2, test_size=0.2,\n",
        "                                     stratify=df_2[\"cat_label\"],\n",
        "                                     random_state=42)"
      ],
      "metadata": {
        "id": "rRAvYtK99MKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "jPJ3vDFz9t1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# OneHot for categorical\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "ohe.fit(train_dl_df[cat_cols])\n",
        "\n",
        "# Scale numerics\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_dl_df[num_cols])"
      ],
      "metadata": {
        "id": "LU5-Mo0q9dso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom dataset"
      ],
      "metadata": {
        "id": "cf-iSmSE-iVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProductDataset(Dataset):\n",
        "    def __init__(self, df_2, tokenizer, max_len=128):\n",
        "        self.texts = df_2[text_col].tolist()\n",
        "        self.cats = ohe.transform(df_2[cat_cols])\n",
        "        self.nums = scaler.transform(df_2[num_cols])\n",
        "        self.cat_labels = df_2[\"cat_label\"].tolist()\n",
        "        self.subcat_labels = df_2[\"subcat_label\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer(text, padding=\"max_length\",\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_len,\n",
        "                                  return_tensors=\"pt\")\n",
        "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
        "        item[\"cats\"] = torch.tensor(self.cats[idx], dtype=torch.float)\n",
        "        item[\"nums\"] = torch.tensor(self.nums[idx], dtype=torch.float)\n",
        "        item[\"labels_cat\"] = torch.tensor(self.cat_labels[idx], dtype=torch.long)\n",
        "        item[\"labels_sub\"] = torch.tensor(self.subcat_labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "train_dataset = ProductDataset(train_dl_df, tokenizer)\n",
        "test_dataset = ProductDataset(test_dl_df, tokenizer)"
      ],
      "metadata": {
        "id": "ts3fR8Sn-V85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model (BERT + structured features)"
      ],
      "metadata": {
        "id": "tCA6JeSy_CgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertMultiTask(nn.Module):\n",
        "    def __init__(self, num_cat_labels, num_subcat_labels, cat_dim, num_dim):\n",
        "        super(BertMultiTask, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "        # Extra features (categorical + numeric)\n",
        "        self.cat_fc = nn.Linear(cat_dim, 128)\n",
        "        self.num_fc = nn.Linear(num_dim, 64)\n",
        "\n",
        "        # Two classification heads\n",
        "        self.classifier_category = nn.Linear(hidden_size + 128 + 64, num_cat_labels)\n",
        "        self.classifier_subcat = nn.Linear(hidden_size + 128 + 64, num_subcat_labels)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, cats, nums, labels_cat=None, labels_sub=None):\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Process structured features\n",
        "        cat_out = self.relu(self.cat_fc(cats))\n",
        "        num_out = self.relu(self.num_fc(nums))\n",
        "\n",
        "        # Merge all\n",
        "        combined = torch.cat((pooled_output, cat_out, num_out), dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        logits_cat = self.classifier_category(combined)\n",
        "        logits_sub = self.classifier_subcat(combined)\n",
        "\n",
        "        loss = None\n",
        "        if labels_cat is not None and labels_sub is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits_cat, labels_cat) + loss_fct(logits_sub, labels_sub)\n",
        "\n",
        "        return {\"loss\": loss, \"logits_cat\": logits_cat, \"logits_sub\": logits_sub}"
      ],
      "metadata": {
        "id": "DklRVXGnM10T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Setup"
      ],
      "metadata": {
        "id": "BrFfj4Si_zyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertMultiTask(\n",
        "    num_cat_labels=num_cat_labels,\n",
        "    num_subcat_labels=num_subcat_labels,\n",
        "    cat_dim=ohe.transform(train_dl_df[cat_cols]).shape[1],\n",
        "    num_dim=len(num_cols)\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "eLWlONcX_0yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "Fzm_Y6fhA5dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "gSeSqYZzB8O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs[\"loss\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "xrEgvtoCCAPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation"
      ],
      "metadata": {
        "id": "Pq09c5kTIdbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds_cat, all_preds_sub, all_labels_cat, all_labels_sub = [], [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        preds_cat = torch.argmax(outputs[\"logits_cat\"], dim=1)\n",
        "        preds_sub = torch.argmax(outputs[\"logits_sub\"], dim=1)\n",
        "\n",
        "        all_preds_cat.extend(preds_cat.cpu().numpy())\n",
        "        all_preds_sub.extend(preds_sub.cpu().numpy())\n",
        "\n",
        "        all_labels_cat.extend(batch[\"labels_cat\"].cpu().numpy())\n",
        "        all_labels_sub.extend(batch[\"labels_sub\"].cpu().numpy())\n"
      ],
      "metadata": {
        "id": "FJ7y0XsTIkeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nCategory Report:\\n\", classification_report(all_labels_cat, all_preds_cat, target_names=cat_encoder.classes_))"
      ],
      "metadata": {
        "id": "Fl9uT5WxgT6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSub-Category Report:\\n\", classification_report(\n",
        "    all_labels_sub,\n",
        "    all_preds_sub,\n",
        "    labels=np.unique(all_labels_sub),\n",
        "    target_names=subcat_encoder.classes_[np.unique(all_labels_sub)]\n",
        "))"
      ],
      "metadata": {
        "id": "Yg-neD1_gXG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictiton"
      ],
      "metadata": {
        "id": "M3SOoEEqIq8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, sub_category, brand, type_, sale_price, market_price):\n",
        "    # Build dataframe for preprocessing\n",
        "    df_sample = pd.DataFrame({\n",
        "        \"description\": [text],\n",
        "        \"sub_category\": [sub_category],\n",
        "        \"brand\": [brand],\n",
        "        \"type\": [type_],\n",
        "        \"sale_price\": [sale_price],\n",
        "        \"market_price\": [market_price]\n",
        "    })\n",
        "\n",
        "    # Process categorical + numerical features\n",
        "    cats = torch.tensor(ohe.transform(df_sample[cat_cols]), dtype=torch.float)\n",
        "    nums = torch.tensor(scaler.transform(df_sample[num_cols]), dtype=torch.float)\n",
        "\n",
        "    # Process text\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=128,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Move tensors to device\n",
        "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
        "    cats, nums = cats.to(device), nums.to(device)\n",
        "\n",
        "    # Prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding, cats=cats, nums=nums)\n",
        "        pred_cat = torch.argmax(outputs[\"logits_cat\"], dim=1).item()\n",
        "        pred_sub = torch.argmax(outputs[\"logits_sub\"], dim=1).item()\n",
        "\n",
        "    # Decode labels\n",
        "    category = cat_encoder.inverse_transform([pred_cat])[0]\n",
        "    subcategory = subcat_encoder.inverse_transform([pred_sub])[0]\n",
        "\n",
        "    return category, subcategory\n"
      ],
      "metadata": {
        "id": "A8_a-SZ7ItnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Beverage Example\n",
        "cat, subcat = predict(\"Organic green tea with natural flavor\",\n",
        "                      \"Tea\", \"Organic India\", \"Beverages\", 150, 180)\n",
        "print(\"ðŸ”® Example 1 â†’ Category:\", cat, \"| Sub-Category:\", subcat)\n",
        "\n",
        "# 2. High-Value Electronics Example\n",
        "cat, subcat = predict(\"Latest model smartphone with 128GB storage and triple camera system.\",\n",
        "                      \"Mobile Phones\", \"Samsung\", \"Electronics\", 45000, 50000)\n",
        "print(\"ðŸ”® Example 2 â†’ Category:\", cat, \"| Sub-Category:\", subcat)\n",
        "\n",
        "# 3. Apparel Example\n",
        "cat, subcat = predict(\"100% Cotton blue denim jeans for men. Regular fit, 32 inch waist.\",\n",
        "                      \"Jeans\", \"Levis\", \"Clothing\", 2500, 3500)\n",
        "print(\"ðŸ”® Example 3 â†’ Category:\", cat, \"| Sub-Category:\", subcat)\n",
        "\n",
        "# 4. Low-Value Consumable/Pantry Example\n",
        "cat, subcat = predict(\"Pure refined sunflower oil for cooking and deep frying, 1 liter bottle.\",\n",
        "                      \"Edible Oils\", \"Fortune\", \"Grocery\", 120, 140)\n",
        "print(\"ðŸ”® Example 4 â†’ Category:\", cat, \"| Sub-Category:\", subcat)\n",
        "\n",
        "# 5. Beauty/Cosmetics Example\n",
        "cat, subcat = predict(\"SPF 50+ Sunscreen for oily skin. Non-comedogenic and dermatologically tested.\",\n",
        "                      \"Sun Protection\", \"L'Oreal\", \"Beauty\", 850, 1000)\n",
        "print(\"ðŸ”® Example 5 â†’ Category:\", cat, \"| Sub-Category:\", subcat)\n",
        "\n",
        "# 6. Home Goods/Furniture Example\n",
        "cat, subcat = predict(\"Foldable study table made of engineered wood. Perfect for small apartments.\",\n",
        "                      \"Study Table\", \"Home Centre\", \"Furniture\", 5500, 7000)\n",
        "print(\"ðŸ”® Example 6 â†’ Category:\", cat, \"| Sub-Category:\", subcat)\n"
      ],
      "metadata": {
        "id": "KQjk9Uh2JD7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"category\"].unique()"
      ],
      "metadata": {
        "id": "LoPhg_5QI90c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommendation system"
      ],
      "metadata": {
        "id": "gmUyMoARSjgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part aims to make a recommendation system to give product suggestion to customers.  "
      ],
      "metadata": {
        "id": "85B4eQboSnv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global configuration"
      ],
      "metadata": {
        "id": "Rf2svr7-gHbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy 'df' dataframe into new data\n",
        "df_3 = df.copy()\n",
        "\n",
        "PRICE_COLUMN = 'sale_price'\n",
        "TEXT_COLUMN = 'description'\n",
        "CATEGORY_COLUMN = 'category'\n",
        "BRAND_COLUMN = 'brand'\n",
        "RATING_COLUMN = 'rating'\n",
        "TYPE_COLUMN = 'type'\n",
        "EMB_FINAL_COL = 'emb_fused'\n",
        "\n",
        "STRUCTURAL_COLS = [CATEGORY_COLUMN, BRAND_COLUMN, TYPE_COLUMN]\n",
        "NUMERIC_COLS = [PRICE_COLUMN, RATING_COLUMN, 'market_price', 'price_diff']\n",
        "\n",
        "# Use a fast, performant Sentence Transformer model\n",
        "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "\n",
        "# ENHANCEMENT: Fusion Weights (can be tuned)\n",
        "FUSION_WEIGHT_STRUCT = 0.5\n",
        "FUSION_WEIGHT_NUM = 0.35\n",
        "\n",
        "# ENHANCEMENT: Confidence thresholds for category filtering\n",
        "CATEGORY_STRICT_THRESHOLD = 0.75\n",
        "CATEGORY_SOFT_THRESHOLD = 0.55\n",
        "CATEGORY_SOFT_TOP_N = 3"
      ],
      "metadata": {
        "id": "cCQ4BpHlKaUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(MODEL_NAME)"
      ],
      "metadata": {
        "id": "YzW7PHxUk-OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define helper functions"
      ],
      "metadata": {
        "id": "S_a01FgukdOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper: parse price constraint\n",
        "def parse_price_constraint(query):\n",
        "    \"\"\"Parses price constraints (under X, between X and Y, etc.) from the query.\"\"\"\n",
        "    q = query.lower()\n",
        "    # between X and Y\n",
        "    m = re.search(r'between\\s*([\\d,]+)\\s*(and|to|-)\\s*([\\d,]+)', q)\n",
        "    if m:\n",
        "        a, b = float(m.group(1).replace(',', '')), float(m.group(3).replace(',', ''))\n",
        "        return min(a, b), max(a, b), query.replace(m.group(0), '').strip()\n",
        "    # under / below / less than\n",
        "    m = re.search(r'(under|below|less than|max)\\s*([\\d,]+)', q)\n",
        "    if m:\n",
        "        v = float(m.group(2).replace(',', ''))\n",
        "        return None, v, query.replace(m.group(0), '').strip()\n",
        "    # over / above / more than\n",
        "    m = re.search(r'(over|above|more than|min)\\s*([\\d,]+)', q)\n",
        "    if m:\n",
        "        v = float(m.group(2).replace(',', ''))\n",
        "        return v, None, query.replace(m.group(0), '').strip()\n",
        "    return None, None, query"
      ],
      "metadata": {
        "id": "ZfPcEHStkaoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Emdedding"
      ],
      "metadata": {
        "id": "ryTFWR0rkoN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_embeddings_with_features(df_input, model, normalize=True):\n",
        "    \"\"\"\n",
        "    Enhanced version: preprocesses all structural and numerical features,\n",
        "    integrates them with text embeddings, and builds the FAISS index.\n",
        "    \"\"\"\n",
        "    df_3 = df_input.copy()\n",
        "\n",
        "    # ... (Imputation and Preprocessing of df_3 remains the same) ...\n",
        "    # 1. Feature Engineering and Robust Imputation (CRUCIAL)\n",
        "    df_3['price_diff'] = df_3['market_price'] - df_3['sale_price']\n",
        "\n",
        "    # Imputation: Fill missing RATING with the median\n",
        "    df_3[RATING_COLUMN] = df_3[RATING_COLUMN].fillna(df_3[RATING_COLUMN].median())\n",
        "    df_3['market_price'] = df_3['market_price'].fillna(df_3['sale_price'])\n",
        "    df_3[NUMERIC_COLS] = df_3[NUMERIC_COLS].fillna(0)\n",
        "\n",
        "    df_3[STRUCTURAL_COLS] = df_3[STRUCTURAL_COLS].fillna(\"Unknown\")\n",
        "    df_3[TEXT_COLUMN] = df_3[TEXT_COLUMN].fillna(\"\")\n",
        "\n",
        "    # 2. Preprocessing Objects (Initialize and Fit)\n",
        "    scaler = StandardScaler()\n",
        "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "    # Fit and transform\n",
        "    num_scaled = scaler.fit_transform(df_3[NUMERIC_COLS])\n",
        "    struct_onehot = encoder.fit_transform(df_3[STRUCTURAL_COLS])\n",
        "\n",
        "    # 3. Text embeddings\n",
        "    text_embs = model.encode(df_3[TEXT_COLUMN].tolist(),\n",
        "                             convert_to_numpy=True,\n",
        "                             normalize_embeddings=False,\n",
        "                             show_progress_bar=True)\n",
        "\n",
        "    # 4. Concatenate (Text + Scaled Numerics + Structured OHE)\n",
        "    fused = np.hstack([\n",
        "        text_embs,\n",
        "        struct_onehot * FUSION_WEIGHT_STRUCT,\n",
        "        num_scaled * FUSION_WEIGHT_NUM\n",
        "    ])\n",
        "\n",
        "    # 5. FIX: Ensure array is float32 and contiguous before normalization/FAISS\n",
        "    fused = fused.astype('float32')\n",
        "\n",
        "    if normalize:\n",
        "        faiss.normalize_L2(fused) # FIX: The input 'fused' is now guaranteed float32\n",
        "\n",
        "    # 6. Store embeddings and build FAISS index\n",
        "    df_3[EMB_FINAL_COL] = list(fused)\n",
        "    dim = fused.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index = faiss.IndexIDMap(index)\n",
        "    ids = df_3.index.to_numpy().astype('int64')\n",
        "\n",
        "    # Ensure add_with_ids input is also float32 (already done, but good to check)\n",
        "    index.add_with_ids(fused, ids)\n",
        "\n",
        "    # Calculate Neutral Structured/Numerical Vector for Query\n",
        "    mode_struct_row = df_3[STRUCTURAL_COLS].mode().iloc[0]\n",
        "    neutral_struct_df = pd.DataFrame([mode_struct_row], columns=STRUCTURAL_COLS)\n",
        "    neutral_struct_onehot = encoder.transform(neutral_struct_df)\n",
        "\n",
        "    neutral_num_scaled = np.zeros((1, len(NUMERIC_COLS)))\n",
        "\n",
        "    # Fused Neutral Query Vector (must also be float32)\n",
        "    fused_neutral_vector_raw = np.hstack([\n",
        "        neutral_struct_onehot * FUSION_WEIGHT_STRUCT,\n",
        "        neutral_num_scaled * FUSION_WEIGHT_NUM\n",
        "    ])\n",
        "    fused_neutral_vector = fused_neutral_vector_raw.astype('float32') # Ensure compatibility\n",
        "\n",
        "    return df_3, index, encoder, scaler, fused_neutral_vector"
      ],
      "metadata": {
        "id": "_HDhDfE1kigu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Search function (with semantic + rule filters)"
      ],
      "metadata": {
        "id": "1CHBDflflQBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIXED ---\n",
        "def hybrid_search(\n",
        "    user_query: str,\n",
        "    df_3: pd.DataFrame,\n",
        "    model,\n",
        "    index,\n",
        "    encoder,\n",
        "    scaler, # Included for completeness\n",
        "    fused_neutral_vector,\n",
        "    k: int = 5,\n",
        "    price_column: str = PRICE_COLUMN,\n",
        "    category_column: str = CATEGORY_COLUMN,\n",
        "    verbose: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs multi-stage search with relaxed filtering (fallback mechanism)\n",
        "    to prevent premature loss of products and filters the global fallback.\n",
        "    \"\"\"\n",
        "    # --- 1. Extract price filter ---\n",
        "    min_price, max_price, clean_query = parse_price_constraint(user_query)\n",
        "    filtered = df_3.copy()\n",
        "\n",
        "    # Store the set BEFORE price filtering, for potential fallback\n",
        "    original_set = filtered.copy()\n",
        "\n",
        "    # Apply Price Filter\n",
        "    if min_price is not None:\n",
        "        filtered = filtered[filtered[price_column].astype(float) >= min_price]\n",
        "    if max_price is not None:\n",
        "        filtered = filtered[filtered[price_column].astype(float) <= max_price]\n",
        "\n",
        "    if filtered.empty:\n",
        "        # ðŸ›¡ï¸ FALLBACK 1: If price filter yields zero, revert to the original set\n",
        "        if verbose: print(\"[WARNING] Price filter too strict. Reverting to pre-price filter set.\")\n",
        "        filtered = original_set.copy()\n",
        "\n",
        "    # Store the set AFTER price filtering\n",
        "    set_after_price_filter = filtered.copy()\n",
        "\n",
        "    # --- 2. Semantic category intent parser (Enhanced Logic) ---\n",
        "    q_emb = model.encode([clean_query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
        "\n",
        "    categories = filtered[category_column].dropna().unique().tolist()\n",
        "\n",
        "    # Track categories that passed the initial semantic intent check (for Fix 3)\n",
        "    allowed_categories = []\n",
        "\n",
        "    if categories:\n",
        "        cat_embs = model.encode(categories, convert_to_numpy=True, normalize_embeddings=True)\n",
        "        sims = np.dot(cat_embs, q_emb)\n",
        "        best_sim = np.max(sims)\n",
        "\n",
        "        # Determine categories to allow based on similarity\n",
        "        if best_sim > CATEGORY_STRICT_THRESHOLD:\n",
        "            # High confidence: Strict filter\n",
        "            best_cat_idx = int(np.argmax(sims))\n",
        "            best_cat = categories[best_cat_idx]\n",
        "            filtered = filtered[filtered[category_column] == best_cat]\n",
        "            allowed_categories = [best_cat]\n",
        "            if verbose: print(f\"[CATEGORY FILTER] STICKY: '{best_cat}' (score={best_sim:.3f})\")\n",
        "        elif best_sim > CATEGORY_SOFT_THRESHOLD:\n",
        "            # Moderate confidence: Soft filter (top N categories)\n",
        "            top_cat_indices = np.argsort(sims)[-CATEGORY_SOFT_TOP_N:]\n",
        "            top_cats = [categories[i] for i in top_cat_indices]\n",
        "            filtered = filtered[filtered[category_column].isin(top_cats)]\n",
        "            allowed_categories = top_cats\n",
        "            if verbose: print(f\"[CATEGORY FILTER] SOFT: Top {len(top_cats)} categories applied (Max Score={best_sim:.3f})\")\n",
        "\n",
        "    if filtered.empty:\n",
        "        # ðŸ›¡ï¸ FALLBACK 2: If category filter yields zero, revert to the set that passed the price filter\n",
        "        if verbose: print(\"[WARNING] Category filter too strict. Reverting to set after price filter.\")\n",
        "        filtered = set_after_price_filter.copy()\n",
        "\n",
        "    if filtered.empty:\n",
        "        if verbose: print(\"[INFO] Filter criteria resulted in zero products.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # --- 3. Encode query with FIX 1: Zero-Out Structural Bias ---\n",
        "\n",
        "    # Calculate a zero vector matching the size of the structural/numerical part\n",
        "    non_text_dim = fused_neutral_vector.shape[1]\n",
        "    zero_non_text_vector = np.zeros((1, non_text_dim), dtype=np.float32)\n",
        "\n",
        "    # Augment text embedding with the zero vector (only semantic push)\n",
        "    text_emb = model.encode([clean_query], convert_to_numpy=True, normalize_embeddings=False)\n",
        "    fused_query_base = np.hstack([text_emb, zero_non_text_vector])\n",
        "    faiss.normalize_L2(fused_query_base)\n",
        "\n",
        "    # --- 4. Search FAISS index ---\n",
        "    D, I = index.search(fused_query_base.astype('float32'), k * 10)\n",
        "    results = []\n",
        "    valid_ids = set(filtered.index.to_numpy().astype('int64'))\n",
        "\n",
        "    for score, idx in zip(D[0], I[0]):\n",
        "        if idx in valid_ids:\n",
        "            item = df_3.loc[idx].to_dict()\n",
        "            item['similarity_score'] = float(score)\n",
        "            results.append(item)\n",
        "            if len(results) >= k:\n",
        "                break\n",
        "\n",
        "    if not results:\n",
        "        # ðŸ›¡ï¸ FALLBACK 3: Global search with category filtering (FIX 3)\n",
        "        if verbose:\n",
        "            print(\"[WARNING] Vector search failed on filtered set. Re-searching original set globally...\")\n",
        "\n",
        "        D_full, I_full = index.search(fused_query_base.astype('float32'), k * 20) # Search top 20 globally\n",
        "\n",
        "        results_full = []\n",
        "\n",
        "        for score, idx in zip(D_full[0], I_full[0]):\n",
        "             item = df_3.loc[idx].to_dict()\n",
        "\n",
        "             # FIX 3: Global Fallback Filter\n",
        "             # Only accept the product if its category was deemed plausible (allowed_categories)\n",
        "             # OR if no category filter was applied at all (empty allowed_categories list).\n",
        "             is_plausible = not allowed_categories or (item[category_column] in allowed_categories)\n",
        "\n",
        "             if is_plausible:\n",
        "                 item['similarity_score'] = float(score)\n",
        "                 results_full.append(item)\n",
        "\n",
        "             if len(results_full) >= k:\n",
        "                 break\n",
        "\n",
        "        if results_full:\n",
        "             if verbose: print(\"[INFO] Returning results from filtered global search.\")\n",
        "             recs = pd.DataFrame(results_full).sort_values(by='similarity_score', ascending=False).head(k)\n",
        "             return recs\n",
        "        else:\n",
        "             if verbose: print(\"[INFO] Global search also failed. Returning empty.\")\n",
        "             return pd.DataFrame()\n",
        "\n",
        "    recs = pd.DataFrame(results).sort_values(by='similarity_score', ascending=False).head(k)\n",
        "    return recs"
      ],
      "metadata": {
        "id": "QKLUeOwKlPy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "zdqDhjPPdrih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Setup and Embeddings ---\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    # This line now returns all the necessary preprocessing objects\n",
        "    df_3, index, encoder, scaler, fused_neutral_vector = build_embeddings_with_features(df_3, model)\n",
        "\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"I need an organic breakfast item under 1000\",\n",
        "        \"I'm looking for a powerful laptop over 25000\",\n",
        "        \"soft fabric shirt for men\",\n",
        "        \"A reliable cream with high rating\" # New query testing rating influence\n",
        "    ]\n",
        "\n",
        "    for q in queries:\n",
        "        print(f\"\\nQuery: {q}\")\n",
        "        # The hybrid_search function now uses all the returned objects\n",
        "        recs = hybrid_search(q, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "        if not recs.empty:\n",
        "            print(recs[[CATEGORY_COLUMN, BRAND_COLUMN, PRICE_COLUMN, RATING_COLUMN, 'similarity_score']])"
      ],
      "metadata": {
        "id": "VA0wsCs6ns_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Testing Enhanced Hybrid Search Assistant (Domain-Specific) ---\")\n",
        "\n",
        "# Define the output columns to show all relevant features\n",
        "OUTPUT_COLS = ['category', 'sub_category', 'brand', PRICE_COLUMN, RATING_COLUMN, 'similarity_score']\n",
        "\n",
        "# --- New Domain-Specific Queries ---\n",
        "\n",
        "# 1. SOFT Filter Test (Beverages)\n",
        "query_d1 = \"instant coffee\"\n",
        "print(f\"\\nQuery 1: '{query_d1}'\")\n",
        "recs_d1 = hybrid_search(query_d1, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "if not recs_d1.empty:\n",
        "    print(recs_d1[OUTPUT_COLS])\n",
        "\n",
        "# 2. STICKY Filter Test (Cleaning & Household)\n",
        "query_d2 = \"laundry liquid\"\n",
        "print(f\"\\nQuery 2: '{query_d2}'\")\n",
        "recs_d2 = hybrid_search(query_d2, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "if not recs_d2.empty:\n",
        "    print(recs_d2[OUTPUT_COLS])\n",
        "\n",
        "# 3. Rating and Semantic Test (Snacks & Branded Foods)\n",
        "query_d3 = \"The best cheap biscuits\"\n",
        "print(f\"\\nQuery 3: '{query_d3}'\")\n",
        "recs_d3 = hybrid_search(query_d3, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "if not recs_d3.empty:\n",
        "    print(recs_d3[OUTPUT_COLS])\n",
        "\n",
        "# 4. Dual Price Range Test (Gourmet & World Food)\n",
        "query_d4 = \"cheese between 200 and 1000\"\n",
        "print(f\"\\nQuery 4: '{query_d4}'\")\n",
        "recs_d4 = hybrid_search(query_d4, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "if not recs_d4.empty:\n",
        "    print(recs_d4[OUTPUT_COLS])\n",
        "\n",
        "# 5. Null/Imputed Data Test (Baby Care)\n",
        "query_d5 = \"Cheap baby care essentials\"\n",
        "print(f\"\\nQuery 5: '{query_d5}'\")\n",
        "recs_d5 = hybrid_search(query_d5, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "if not recs_d5.empty:\n",
        "    print(recs_d5[OUTPUT_COLS])\n",
        "\n",
        "# 6. Simple Category Search (Eggs, Meat & Fish)\n",
        "query_d6 = \"Fresh meat products\"\n",
        "print(f\"\\nQuery 6: '{query_d6}'\")\n",
        "recs_d6 = hybrid_search(query_d6, df_3, model, index, encoder, scaler, fused_neutral_vector, k=3, verbose=True)\n",
        "if not recs_d6.empty:\n",
        "    print(recs_d6[OUTPUT_COLS])"
      ],
      "metadata": {
        "id": "dXNzw5PmdeJL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}